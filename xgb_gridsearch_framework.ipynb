{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB trainging code framework:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ***shell scripting*** with ***python code*** to do XGB grid searching training and output all training log and scoring results for visulization or comparasion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "#### 1. shell scripting: ####\n",
    "#############################\n",
    "### semi-manual grid search using shell scripting #####\n",
    "### output all scoring results for each combination ###\n",
    "\n",
    "#!/bin/bash\n",
    "set -euo pippfail\n",
    "\n",
    "#default parameters if not specified\n",
    "n_estimators=200\n",
    "depth=4\n",
    "reg_lambda=100\n",
    "scale_pos_weight=1\n",
    "\n",
    "#define/check options and parameters\n",
    "while [$# -gt 0 ] ; do\n",
    "    case \"$1\" in \n",
    "        -n) n_estimators=$2; shift;;\n",
    "        -d) depth=$2; shift;;\n",
    "        -l) reg_lambda=$2; shift;;\n",
    "        -s) scale_pos_weight=$2; shift;;\n",
    "        -h) echo \"Set tuning parameters:\"\n",
    "            echo \"Options: -n number of trees\"\n",
    "            echo \"         -d depth\"\n",
    "            echo \"         -l regularization lambda\"\n",
    "            echo \"         -s scale_pos_weight\"\n",
    "            exit;;\n",
    "    esac\n",
    "    shift\n",
    "done\n",
    "       \n",
    "dir='pwd'\n",
    "train_param=n${n_estimators}_d${depth}_l${reg_lambda}_s${scale_pos_weight}\n",
    "score_file=${dir}/XGBTrainScores_${train_param}.dat\n",
    "       \n",
    "#### Train XGB one by one ### \n",
    "train_template=${dir}/trainXGB_template.py\n",
    "\n",
    "# modify template with hyperparameters for this round\n",
    "cat $train_template|sed \"s/_NTREE_/$n_estimators/g;s/_DEPTH_/$depth/g;s/_LAMBDA_/$reg_lambda/g;s/_SCALE_/$scale_pos_weight/g\" > ${dir}/trainXGB.py\n",
    "# run the python script and save the outputs       \n",
    "python trainXGB.py -t train.dat -m XGBtrainScores.dat -f featureMap.txt > training_${train_param}.log\n",
    "mv XGBtrainScores.dat XGBtrainScores${train_param}.dat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "#### 2. trainXGB_template.py ######\n",
    "###################################\n",
    "\n",
    "# trainXGB_template.py\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "import pdb\n",
    "\n",
    "#set display formats\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "#define functions\n",
    "def printVarImport(varImportMap):\n",
    "    idx=0\n",
    "    for key, value in sorted(varImportmap.iteritems(), key=lambda (k,v): (v,k), reverse=True):\n",
    "        if idx < 200:\n",
    "            print('|'.join([key,str(value)]))\n",
    "        else:\n",
    "            print \"...\"\n",
    "            break\n",
    "        idx += 1\n",
    "\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description =\n",
    "             \"\"\"\n",
    "             read in pipe delimited data and train model\n",
    "             \"\"\")\n",
    "    parser.add_argument('-t','--trainDat',required=True, dest='trainDat',help='train data file')\n",
    "    parser.add_argument('-s','--testDat',required=False, dest='testDat',help='test data file to be scored')\n",
    "    parser.add_argument('-v','--valDat',required=False, dest='valDat',help='validation data file to be scored')\n",
    "    parser.add_argument('-o','--outTestScore',required=False, dest='outTestScore',help='output test score file')\n",
    "    parser.add_argument('-m','--outTrainScore',required=False, dest='outTrainScore',help='output train score file')\n",
    "    parser.add_argument('-z','--outValScore',required=False, dest='outValScore',help='output validation score file')\n",
    "    parser.add_argument('-f','--featureMap',required=True, dest='featureMap',help='feature map as model inputs')\n",
    "                       \n",
    "        \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    trainFile = args.trainDat\n",
    "    testFile = args.testDat\n",
    "    valFile = args.valDat\n",
    "    outTestScore = args.outTestScore\n",
    "    outTrainScore = args.outTrainScore   \n",
    "    outValScore = args.outValScore\n",
    "    train_fmap = args.featureMap\n",
    "\n",
    "    train = np.genfromtxt(tranFile, delimiter='|', comments=None, missing_values=('.','',' '), filling_values=(np.nan))\n",
    "    trainY = train[0:1] # second column is target\n",
    "    trainX = train[0:,3:] # starting from 4th column\n",
    "    wgt = train[0:,2] # weight is 2nd column\n",
    "\n",
    "    # Fix a few hyperparameters here. Could add more into tweaking list\n",
    "    learning_rate = 0.1\n",
    "    gamma = 0\n",
    "    subsample = 0.7\n",
    "    colsample_bytree= 0.7\n",
    "    min_child_weight = 50\n",
    "    eta = 0.1\n",
    "    \n",
    "    n_estimators = _NTREE_\n",
    "    max_depth = _DEPTH_\n",
    "    reg_lambda = _LAMBDA_\n",
    "    scale_pos_weight = _SCALE_ # for unbalanced data\n",
    "    \n",
    "    seed = 27\n",
    "    nthread = 4\n",
    "    cv_folds = 3\n",
    "    early_stopping_rounds = 50\n",
    "\n",
    "    # train classification trees, using AUC as the criteria\n",
    "    xgb_class = xgb.XGBClassifier(learning_rate = learning_rate,\n",
    "                                 n_estimators = n_estimators,\n",
    "                                 max_depth = max_depth,\n",
    "                                 min_child_weight = min_child_weight,\n",
    "                                 reg_lambda = reg_lambda,\n",
    "                                 subsample = subsample,\n",
    "                                 colsample_bytree =colsample_bytree,\n",
    "                                 objective='binary:logistic',\n",
    "                                 scale_pos_weight = scale_pos_weight,\n",
    "                                 nthread = nthread,\n",
    "                                 seed = seed)\n",
    "    xgb_param = xgb_class.get_xgb_params()\n",
    "    xgtrain = xgb.DMatrix(trainX, label=trainY,weight = wgt)\n",
    "    cvresult = xgb.cv(xgb_param,xgtrain, num_boost_round = n_estimators, nfold=cv_folds,metrics='auc',early_stopping_rounds=early_stopping_rounds)\n",
    "    cvresult['diff-auc'] = cvresult['train-auc-mean'] - cvresult['test-auc-mean']\n",
    "    xgb_class.fit(trainX, trainY,eval_metric='auc')\n",
    "    \n",
    "    # preditions\n",
    "    dtrain_predictions = xgb_class.predict(trainX)\n",
    "    dtrain_predprob = xgb_class.predict_proba(trainX)[:,1]\n",
    "    \n",
    "    # variable importance\n",
    "    gainVarImport = xgb_class.get_booster().get_score(fmap=train_fmap, importance_type='gain')\n",
    "    coverVarImport = xgb_class.get_booster().get_score(fmap=train_fmap, importance_type='cover')\n",
    "       \n",
    "    print \"Top variables by Gain Importance\"\n",
    "    printVarImport(gainVarImport)\n",
    "    print \n",
    "    \n",
    "    print \"Top variables by Cover Importance\"\n",
    "    printVarImport(coverVarImport)\n",
    "    print \n",
    "    \n",
    "    clf = xgb_class.get_booster()\n",
    "    clf.dump_model('xgbModel.txt', fmap=train_fmap)\n",
    "    pickle.dump(clf,open(\"xgb.pickle\",\"wb\"))\n",
    "\n",
    "    \n",
    "    if outTrainScore:\n",
    "        print \"score training data...\"\n",
    "        seqnumTrain = train[0:,0]\n",
    "        dtrain = xgb.DMatrix(trainX, label=trainY)\n",
    "        scoreTrain = clf.predict(dtrain)\n",
    "        \n",
    "        fTrain = open(outTrainScore,'w')\n",
    "        writerTrain = csv.writer(fTrain, lineterminator='\\n')\n",
    "        for i in range(len(seqnumTrain)):\n",
    "            int_score = int(max(1,min(999,1000*(1-scoreTrain[i]))))\n",
    "            writerTrain.writerow([(\"%.0f\" %seqnumTrain[i]),trainY[i],wgt[i],scoreTrain[i],int_score])\n",
    "            \n",
    "    if outTestScore:\n",
    "        print \"score test data...\"\n",
    "        scoretestData = np.genfromtxt(testFile, delimiter='|', comments=None, missing_values=('.','',' '), filling_values=(np.nan))\n",
    "        scoretestY = scoretestData[0:1] # second column is target\n",
    "        scoretestX = scoretestData[0:,3:] # starting from 4th column\n",
    "        testwgt = scoretestData[0:,2] # weight is 2nd column\n",
    "        testseqnum = scoretestData[0:,0]\n",
    "        \n",
    "        dtest = xgb.DMatrix(scoretestX, label=None)\n",
    "        testscore = clf.predict(dtest)\n",
    "        \n",
    "        f = open(outTestScore,'w')\n",
    "        writerTrain = csv.writer(f, lineterminator='\\n')\n",
    "        for i in range(len(testseqnum)):\n",
    "            int_score = int(max(1,min(999,1000*(1-testscore[i]))))\n",
    "            writer.writerow([(\"%.0f\" %testseqnum[i]),scoretestX[i],testwgt[i],scoretestY[i],int_score])\n",
    "            \n",
    "              "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
